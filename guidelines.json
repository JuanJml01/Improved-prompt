{
  "introduction": "Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently apply and build with large language models (LLMs) for a wide variety of applications and use cases.\nPrompt engineering skills help to better understand the capabilities and limitations of LLMs. Researchers use prompt engineering to improve safety and the capacity of LLMs on a wide range of common and complex tasks such as question answering and arithmetic reasoning. Developers use prompt engineering to design robust and effective prompting techniques that interface with LLMs and other tools.\nThis comprehensive guide covers the theory and practical aspects of prompt engineering and how to leverage the best prompting techniques to interact and build with LLMs.\n# Elements of a Prompt\nAs we cover more and more examples and applications with prompt engineering, you will notice that certain elements make up a prompt.\nA prompt contains any of the following elements:\n**Instruction** - a specific task or instruction you want the model to perform\n**Context** - external information or additional context that can steer the model to better responses\n**Input Data** - the input or question that we are interested to find a response for\n**Output Indicator** - the type or format of the output.\nTo demonstrate the prompt elements better, here is a simple prompt that aims to perform a text classification task:\n*Prompt*\n```\nClassify the text into neutral, negative, or positive\nText: I think the food was okay.\nSentiment:\n```\nIn the prompt example above, the instruction correspond to the classification task, \"Classify the text into neutral, negative, or positive\". The input data corresponds to the \"I think the food was okay.' part, and the output indicator used is \"Sentiment:\". Note that this basic example doesn't use context but this can also be provided as part of the prompt. For instance, the context for this text classification prompt can be additional examples provided as part of the prompt to help the model better understand the task and steer the type of outputs that you expect.\nYou do not need all the four elements for a prompt and the format depends on the task at hand. We will touch on more concrete examples in upcoming guides.\n# General Tips for Designing Prompts\nHere are some tips to keep in mind while you are designing your prompts:\n### Start Simple\nAs you get started with designing prompts, you should keep in mind that it is really an iterative process that requires a lot of experimentation to get optimal results. Using a simple playground from OpenAI or Cohere is a good starting point.\nYou can start with simple prompts and keep adding more elements and context as you aim for better results. Iterating your prompt along the way is vital for this reason. As you read the guide, you will see many examples where specificity, simplicity, and conciseness will often give you better results.\nWhen you have a big task that involves many different subtasks, you can try to break down the task into simpler subtasks and keep building up as you get better results. This avoids adding too much complexity to the prompt design process at the beginning.\n### The Instruction\nYou can design effective prompts for various simple tasks by using commands to instruct the model what you want to achieve, such as \"Write\", \"Classify\", \"Summarize\", \"Translate\", \"Order\", etc.\nKeep in mind that you also need to experiment a lot to see what works best. Try different instructions with different keywords, contexts, and data and see what works best for your particular use case and task. Usually, the more specific and relevant the context is to the task you are trying to perform, the better. We will touch on the importance of sampling and adding more context in the upcoming guides.\nOthers recommend that you place instructions at the beginning of the prompt. Another recommendation is to use some clear separator like \"###\" to separate the instruction and context.\nFor instance:\n*Prompt:*\n```\n### Instruction ###\nTranslate the text below to Spanish:\nText: \"hello!\"\n```\n*Output:*\n```\n¡Hola!\n```\n### Specificity\nBe very specific about the instruction and task you want the model to perform. The more descriptive and detailed the prompt is, the better the results. This is particularly important when you have a desired outcome or style of generation you are seeking. There aren't specific tokens or keywords that lead to better results. It's more important to have a good format and descriptive prompt. In fact, providing examples in the prompt is very effective to get desired output in specific formats.\nWhen designing prompts, you should also keep in mind the length of the prompt as there are limitations regarding how long the prompt can be. Thinking about how specific and detailed you should be. Including too many unnecessary details is not necessarily a good approach. The details should be relevant and contribute to the task at hand. This is something you will need to experiment with a lot. We encourage a lot of experimentation and iteration to optimize prompts for your applications.\nAs an example, let's try a simple prompt to extract specific information from a piece of text.\n*Prompt:*\n```\nExtract the name of places in the following text.\nDesired format:\nPlace:\nInput: \"Although these developments are encouraging to researchers, much is still a mystery. “We often have a black box between the brain and the effect we see in the periphery,” says Henrique Veiga-Fernandes, a neuroimmunologist at the Champalimaud Centre for the Unknown in Lisbon. “If we want to use it in the therapeutic context, we actually need to understand the mechanism.“\"\n```\n*Output:*\n```\nPlace: Champalimaud Centre for the Unknown, Lisbon\n```\n### Avoid Impreciseness\nGiven the tips above about being detailed and improving format, it's easy to fall into the trap of wanting to be too clever about prompts and potentially creating imprecise descriptions. It's often better to be specific and direct. The analogy here is very similar to effective communication -- the more direct, the more effective the message gets across.\nFor example, you might be interested in learning the concept of prompt engineering. You might try something like:\n```\nExplain the concept prompt engineering. Keep the explanation short, only a few sentences, and don't be too descriptive.\n```\nIt's not clear from the prompt above how many sentences to use and what style. You might still somewhat get good responses with the above prompts but the better prompt would be one that is very specific, concise, and to the point. Something like:\n```\nUse 2-3 sentences to explain the concept of prompt engineering to a high school student.\n```\n### To do or not to do?\nAnother common tip when designing prompts is to avoid saying what not to do but say what to do instead. This encourages more specificity and focuses on the details that lead to good responses from the model.\nHere is an example of a movie recommendation chatbot failing at exactly what I don't want it to do because of how I wrote the instruction -- focusing on what not to do.\n*Prompt:*\n```\nThe following is an agent that recommends movies to a customer. DO NOT ASK FOR INTERESTS. DO NOT ASK FOR PERSONAL INFORMATION.\nCustomer: Please recommend a movie based on my interests.\nAgent:\n```\n*Output:*\n```\nSure, I can recommend a movie based on your interests. What kind of movie would you like to watch? Do you prefer action, comedy, romance, or something else?\n```\nHere is a better prompt:\n*Prompt:*\n```\nThe following is an agent that recommends movies to a customer. The agent is responsible to recommend a movie from the top global trending movies. It should refrain from asking users for their preferences and avoid asking for personal information. If the agent doesn't have a movie to recommend, it should respond \"Sorry, couldn't find a movie to recommend today.\".\nCustomer: Please recommend a movie based on my interests.\nAgent:\n```\n*Output:*\n```\nSorry, I don't have any information about your interests. However, here's a list of the top global trending movies right now: [list of movies]. I hope you find something you like!\n```",
  "techniques": [
    {
      "name": "Zero-Shot Prompting",
      "summarized": "Technique where the prompt instructs the model directly **without providing examples or demonstrations**. **When to use it:** It's useful for tasks that modern LLMs can perform based on their large-scale training and instruction tuning. **It is recommended to start with this** as a baseline.",
      "complete": "Large language models (LLMs) today, such as GPT-3.5 Turbo, GPT-4, and Claude 3, are tuned to follow instructions and are trained on large amounts of data. Large-scale training makes these models capable of performing some tasks in a \"zero-shot\" manner. Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it.\nWe tried a few zero-shot examples in the previous section. Here is one of the examples (ie., text classification) we used:\n*Prompt:*\n```\nClassify the text into neutral, negative or positive.\nText: I think the vacation is okay.\nSentiment:\n```\n*Output:*\n```\nNeutral\n```\nNote that in the prompt above we didn't provide the model with any examples of text alongside their classifications, the LLM already understands \"sentiment\" -- that's the zero-shot capabilities at work.\nInstruction tuning has been shown to improve zero-shot learning [Wei et al. (2022)](https://arxiv.org/pdf/2109.01652.pdf). Instruction tuning is essentially the concept of finetuning models on datasets described via instructions. Furthermore, [RLHF](https://arxiv.org/abs/1706.03741) (reinforcement learning from human feedback) has been adopted to scale instruction tuning wherein the model is aligned to better fit human preferences. This recent development powers models like ChatGPT. We will discuss all these approaches and methods in upcoming sections.\nWhen zero-shot doesn't work, it's recommended to provide demonstrations or examples in the prompt which leads to few-shot prompting. In the next section, we demonstrate few-shot prompting."
    },
    {
      "name": "Few-Shot Prompting",
      "summarized": "Technique that includes **input/output examples or demonstrations** within the prompt. **When to use it:** **Recommended when zero-shot prompting is not enough** for more complex tasks, offering demonstrations (2–5 examples, or more for difficult tasks) to guide the model, improve performance, and enable in-context learning. It is particularly effective for achieving a desired format or style.",
      "complete": "While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response.\nAccording to [Touvron et al. 2023](https://arxiv.org/pdf/2302.13971.pdf) few shot properties first appeared when models were scaled to a sufficient size [(Kaplan et al., 2020)](https://arxiv.org/abs/2001.08361).\nLet's demonstrate few-shot prompting via an example that was presented in [Brown et al. 2020](https://arxiv.org/abs/2005.14165). In the example, the task is to correctly use a new word in a sentence.\n*Prompt:*\n```markdown\nA \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\nWe were traveling in Africa and we saw these very cute whatpus.\nTo do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n```\n*Output:*\n```\nWhen we won the game, we all started to farduddle in celebration.\n```\nWe can observe that the model has somehow learned how to perform the task by providing it with just one example (i.e., 1-shot). For more difficult tasks, we can experiment with increasing the demonstrations (e.g., 3-shot, 5-shot, 10-shot, etc.).\n- \"the label space and the distribution of the input text specified by the demonstrations are both important (regardless of whether the labels are correct for individual inputs)\"\n- the format you use also plays a key role in performance, even if you just use random labels, this is much better than no labels at all.\n- additional results show that selecting random labels from a true distribution of labels (instead of a uniform distribution) also helps.\nLet's try out a few examples. Let's first try an example with random labels (meaning the labels Negative and Positive are randomly assigned to the inputs):\n*Prompt:*\n```\nThis is awesome! // Negative\nThis is bad! // Positive\nWow that movie was rad! // Positive\nWhat a horrible show! //\n```\n*Output:*\n```\nNegative\n```\nWe still get the correct answer, even though the labels have been randomized. Note that we also kept the format, which helps too. In fact, with further experimentation, it seems the newer GPT models we are experimenting with are becoming more robust to even random formats. Example:\n*Prompt:*\n```\nPositive This is awesome!\nThis is bad! Negative\nWow that movie was rad!\nPositive\nWhat a horrible show! --\n```\n*Output:*\n```\nNegative\n```\nThere is no consistency in the format above but the model still predicted the correct label. We have to conduct a more thorough analysis to confirm if this holds for different and more complex tasks, including different variations of prompts.\n### Limitations of Few-shot Prompting\nStandard few-shot prompting works well for many tasks but is still not a perfect technique, especially when dealing with more complex reasoning tasks. Let's demonstrate why this is the case. Do you recall the previous example where we provided the following task:\n```\nThe odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\nA:\n```\nIf we try this again, the model outputs the following:\n```\nYes, the odd numbers in this group add up to 107, which is an even number.\n```\nThis is not the correct response, which not only highlights the limitations of these systems but that there is a need for more advanced prompt engineering.\nLet's try to add some examples to see if few-shot prompting improves the results.\n*Prompt:*\n```\nThe odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\nA: The answer is False.\nThe odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.\nA: The answer is True.\nThe odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.\nA: The answer is True.\nThe odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\nA: The answer is False.\nThe odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\nA:\n```\n*Output:*\n```\nThe answer is True.\n```\nThat didn't work. It seems like few-shot prompting is not enough to get reliable responses for this type of reasoning problem. The example above provides basic information on the task. If you take a closer look, the type of task we have introduced involves a few more reasoning steps. In other words, it might help if we break the problem down into steps and demonstrate that to the model. More recently, [chain-of-thought (CoT) prompting](https://arxiv.org/abs/2201.11903) has been popularized to address more complex arithmetic, commonsense, and symbolic reasoning tasks.\nOverall, it seems that providing examples is useful for solving some tasks. When zero-shot prompting and few-shot prompting are not sufficient, it might mean that whatever was learned by the model isn't enough to do well at the task. From here it is recommended to start thinking about fine-tuning your models or experimenting with more advanced prompting techniques. Up next we talk about one of the popular prompting techniques called chain-of-thought prompting which has gained a lot of popularity."
    },
    {
      "name": "Chain-of-Thought (CoT) Prompting",
      "summarized": "Technique that enables complex reasoning capabilities by including **intermediate reasoning steps** in the demonstrations. A variation is Zero-shot CoT, which adds the phrase 'Let's think step by step' to the original prompt. **When to use it:** It is useful for achieving better results on **more complex reasoning tasks** (like arithmetic, commonsense, or symbolic reasoning) where basic few-shot prompting is not enough. Zero-shot CoT is particularly useful when not many examples are available to include in the prompt.",
      "complete": "Introduced in [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.\n*Prompt:*\n```\nThe odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\nA: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\nThe odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.\nA: Adding all the odd numbers (17, 19) gives 36. The answer is True.\nThe odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.\nA: Adding all the odd numbers (11, 13) gives 24. The answer is True.\nThe odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\nA: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\nThe odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\nA:\n```\n*Output:*\n```\nAdding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n```\nWow! We can see a perfect result when we provided the reasoning step. In fact, we can solve this task by providing even fewer examples, i.e., just one example seems enough:\n*Prompt:*\n```\nThe odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\nA: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\nThe odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\nA:\n```\n*Output:*\n```\nAdding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n```\nKeep in mind that the authors claim that this is an emergent ability that arises with sufficiently large language models.\n## Zero-shot COT Prompting\nOne recent idea that came out more recently is the idea of [zero-shot CoT](https://arxiv.org/abs/2205.11916) (Kojima et al. 2022) that essentially involves adding \"Let's think step by step\" to the original prompt. Let's try a simple problem and see how the model performs:\n*Prompt:*\n```\nI went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n```\n*Output:*\n```\n11 apples\n```\nThe answer is incorrect! Now Let's try with the special prompt.\n*Prompt:*\n```\nI went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\nLet's think step by step.\n```\n*Output:*\n```\nFirst, you started with 10 apples.\nYou gave away 2 apples to the neighbor and 2 to the repairman, so you had 6 apples left.\nThen you bought 5 more apples, so now you had 11 apples.\nFinally, you ate 1 apple, so you would remain with 10 apples.\n```\nIt's impressive that this simple prompt is effective at this task. This is particularly useful where you don't have too many examples to use in the prompt.\n"
    },
    {
      "name": "Generated Knowledge Prompting",
      "summarized": "Technique where the model is first used to **generate relevant knowledge or information**, and that knowledge is then used as part of the prompt to make a prediction or give a response. **When to use it:** It is beneficial for **improving accuracy in tasks requiring world knowledge or commonsense reasoning**, where LLMs may show limitations without additional context.",
      "complete": "LLMs continue to be improved and one popular technique includes the ability to incorporate knowledge or information to help the model make more accurate predictions.\nUsing a similar idea, can the model also be used to generate knowledge before making a prediction? That's what is attempted in the paper by [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf) -- generate knowledge to be used as part of the prompt. In particular, how helpful is this for tasks such as commonsense reasoning?\nLet's try a simple prompt:\n*Prompt:*\n```\nPart of golf is trying to get a higher point total than others. Yes or No?\n```\n*Output:*\n```\nYes.\n```\nThis type of mistake reveals the limitations of LLMs to perform tasks that require more knowledge about the world. How do we improve this with knowledge generation?\nFirst, we generate a few \"knowledges\":\n*Prompt:*\n```\nInput: Greece is larger than mexico.\nKnowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\nInput: Glasses always fog up.\nKnowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\nInput: A fish is capable of thinking.\nKnowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\nInput: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\nKnowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\nInput: A rock is the same size as a pebble.\nKnowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\nInput: Part of golf is trying to get a higher point total than others.\nKnowledge:\n```\n*Knowledge 1:*\n```\nThe objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n```\n*Knowledge 2:*\n```\nGolf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n```\nWe are using the prompt provided in the paper by [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf).\nThe next step is to integrate the knowledge and get a prediction. I reformatted the question into QA format to guide the answer format.\n*Prompt:*\n```\nQuestion: Part of golf is trying to get a higher point total than others. Yes or No?\nKnowledge: The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\nExplain and Answer:\n```\n*Answer 1 (confidence very high):*\n```\nNo, the objective of golf is not to get a higher point total than others. Rather, the objective is to play a set of holes in the least number of strokes. The total number of strokes is used to determine the winner of the game, not the total number of points.\n```\n```\nQuestion: Part of golf is trying to get a higher point total than others. Yes or No?\nKnowledge: Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\nExplain and Answer:\n```\n*Answer 2 (confidence is a lot lower):*\n```\nYes, part of golf is trying to get a higher point total than others. Each player tries to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n```\nSome really interesting things happened with this example. In the first answer, the model was very confident but in the second not so much. I simplified the process for demonstration purposes but there are a few more details to consider when arriving at the final answer. Check out the paper for more."
    },
    {
      "name": "Prompt Chaining",
      "summarized": "Technique that involves **breaking a complex task into subtasks** and chaining the prompts, where the output generated by one prompt is used as input for the next. **When to use it:** It is useful for **handling complex tasks** that a single detailed prompt might not address well. It improves transparency, control, and reliability of LLM applications and is especially useful in scenarios requiring multiple steps or transformations, such as **document-based question answering systems** or conversational assistants.",
      "complete": "To improve the reliability and performance of LLMs, one of the important prompt engineering techniques is to break tasks into its subtasks. Once those subtasks have been identified, the LLM is prompted with a subtask and then its response is used as input to another prompt. This is what's referred to as prompt chaining, where a task is split into subtasks with the idea to create a chain of prompt operations.\nPrompt chaining is useful to accomplish complex tasks which an LLM might struggle to address if prompted with a very detailed prompt. In prompt chaining, chain prompts perform transformations or additional processes on the generated responses before reaching a final desired state.\nBesides achieving better performance, prompt chaining helps to boost the transparency of your LLM application, increases controllability, and reliability. This means that you can debug problems with model responses much more easily and analyze and improve performance in the different stages that need improvement.\nPrompt chaining is particularly useful when building LLM-powered conversational assistants and improving the personalization and user experience of your applications.\n## Use Cases for Prompt Chaining\n### Prompt Chaining for Document QA\nPrompt chaining can be used in different scenarios that could involve several operations or transformations. For instance, one common use case of LLMs involves answering questions about a large text document. It helps if you design two different prompts where the first prompt is responsible for extracting relevant quotes to answer a question and a second prompt takes as input the quotes and original document to answer a given question. In other words, you will be creating two different prompts to perform the task of answering a question given in a document.\nThe first prompt below extracts the relevant quotes from the document given the question. Note that for simplicity, we have added a placeholder for the document `{{document}}`. To test the prompt you can copy and paste an article from Wikipedia such as this page for [prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering). Due to larger context used for this task, we are using the `gpt-4-1106-preview` model from OpenAI. You can use the prompt with other long-context LLMs like Claude.\nPrompt 1:\n```\nYou are a helpful assistant. Your task is to help answer a question given in a document. The first step is to extract quotes relevant to the question from the document, delimited by ####. Please output the list of quotes using . Respond with \"No relevant quotes found!\" if no relevant quotes were found.\n####\n{{document}}\n####\n```\nHere is a screenshot of the entire prompt including the question which is passed using the `user` role.\nOutput of Prompt 1:\n```\n- Chain-of-thought (CoT) prompting[27]\n- Generated knowledge prompting[37]\n- Least-to-most prompting[38]\n- Self-consistency decoding[39]\n- Complexity-based prompting[41]\n- Self-refine[42]\n- Tree-of-thought prompting[43]\n- Maieutic prompting[45]\n- Directional-stimulus prompting[46]\n- Textual inversion and embeddings[59]\n- Using gradient descent to search for prompts[61][62][63][64]\n- Prompt injection[65][66][67]\n```\nThe quotes that were returned in the first prompt can now be used as input to the second prompt below. Note that you can clean up the quotes a bit more, i.e., remove the citations. Those citations could be removed or utilized as part of another prompt in the chain but you can ignore this for now. The second prompt then takes the relevant quotes extracted by prompt 1 and prepares a helpful response to the question given in the document and those extracted quotes. The second prompt can be the following:\nPrompt 2:\n```\nGiven a set of relevant quotes (delimited by ) extracted from a document and the original document (delimited by ####), please compose an answer to the question. Ensure that the answer is accurate, has a friendly tone, and sounds helpful.\n####\n{{document}}\n####\n- Chain-of-thought (CoT) prompting[27]\n- Generated knowledge prompting[37]\n- Least-to-most prompting[38]\n- Self-consistency decoding[39]\n- Complexity-based prompting[41]\n- Self-refine[42]\n- Tree-of-thought prompting[43]\n- Maieutic prompting[45]\n- Directional-stimulus prompting[46]\n- Textual inversion and embeddings[59]\n- Using gradient descent to search for prompts[61][62][63][64]\n- Prompt injection[65][66][67]\n```\nOutput of Prompt 2:\n```\nThe prompting techniques mentioned in the document include:\n1. Chain-of-thought (CoT) prompting[27]\n2. Generated knowledge prompting[37]\n3. Least-to-most prompting[38]\n4. Self-consistency decoding[39]\n5. Complexity-based prompting[41]\n6. Self-refine[42]\n7. Tree-of-thought prompting[43]\n8. Maieutic prompting[45]\n9. Directional-stimulus prompting[46]\n10. Textual inversion and embeddings[59]\n11. Using gradient descent to search for prompts[61][62][63][64]\n12. Prompt injection[65][66][67]\nEach of these techniques employs unique strategies to enhance or specify the interactions with large language models to produce the desired outcomes.\n```\nAs you can see, simplifying and creating prompt chains is a useful prompting approach where the responses need to undergo several operations or transformations. As an exercise, feel free to design a prompt that removes the citations (e.g., [27]) from the response before sending this as a final response to the user of your application.\nYou can also find more examples of prompt chaining in this [documentation](https://docs.anthropic.com/claude/docs/prompt-chaining) that leverages the Claude LLM. Our example is inspired and adapted from their examples."
    }
  ]
}